{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-attention Generative Adversarial Capsulet Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import skimage\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from keras import layers, models, optimizers\n",
    "from keras import callbacks, backend\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from utils import load_image, load_faces\n",
    "from discriminator import discriminator_func\n",
    "from generator import generator_func\n",
    "from attention import self_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip\n",
    "\n",
    "with zipfile.ZipFile(\"celeba.zip\",\"r\") as zip_ref:\n",
    "  zip_ref.extractall(\"data_faces/\")\n",
    "\n",
    "root = 'data_faces/img_align_celeba'\n",
    "img_list = os.listdir(root)\n",
    "print(\"Total Number of Images : \", len(img_list))\n",
    "required_size = (64, 64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_images = 50000\n",
    " \n",
    "\n",
    "root = 'data_faces/img_align_celeba/'\n",
    "dataset = load_faces(directory, number_of_images)\n",
    "print('Loaded: ', dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, n_samples):\n",
    "\tix = np.random.randint(0, dataset.shape[0], n_samples)\n",
    "\tX = dataset[ix]\n",
    "\ty = np.ones((n_samples, 1))\n",
    "\treturn X, y\n",
    " \n",
    "\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "\tx_input = np.random.randn(latent_dim * n_samples)\n",
    "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
    "\treturn x_input\n",
    " \n",
    "\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
    "\tX = generator.predict(x_input)\n",
    "\ty = np.zeros((n_samples, 1))\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISCRIMINATOR:\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 124, 124, 16) 1216        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "self_attention_3 (self_attentio (None, 124, 124, 16) 321         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 124, 124, 16) 0           self_attention_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv2 (Conv2DTranspo (None, 128, 128, 32) 12832       leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "self_attention_4 (self_attentio (None, 128, 128, 32) 1281        primarycap_conv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 128, 128, 32) 0           self_attention_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 65536, 8)     0           leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 65536, 8)     0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 524288)       0           primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "uhat_digitcaps (Dense)          (None, 256)          134217984   flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_digitcaps1 (Activation) (None, 256)          0           uhat_digitcaps[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          65792       softmax_digitcaps1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 256)          0           uhat_digitcaps[0][0]             \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 256)          0           multiply_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "softmax_digitcaps2 (Activation) (None, 256)          0           leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          65792       softmax_digitcaps2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_5 (Multiply)           (None, 256)          0           uhat_digitcaps[0][0]             \n",
      "                                                                 dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 256)          0           multiply_5[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "softmax_digitcaps3 (Activation) (None, 256)          0           leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 256)          65792       softmax_digitcaps3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_6 (Multiply)           (None, 256)          0           uhat_digitcaps[0][0]             \n",
      "                                                                 dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 256)          0           multiply_6[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            257         leaky_re_lu_10[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 134,431,267\n",
      "Trainable params: 134,431,267\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /home/nabil/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "input_shape = (128, 128, 3)\n",
    "\n",
    "discriminator = discriminator_func(input_shape)\n",
    "\n",
    "print('DISCRIMINATOR:')\n",
    "\n",
    "discriminator.summary()\n",
    "\n",
    "discriminator.compile(loss = 'binary_crossentropy', optimizer = Adam(0.0004, 0.48), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
